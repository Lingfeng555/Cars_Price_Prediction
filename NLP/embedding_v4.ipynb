{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 18:27:47.300266: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-06 18:27:47.475387: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1730914067.577866   10310 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1730914067.593308   10310 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-06 18:27:47.773236: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/lingfeng/Desktop/tensorflow/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lingfeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "W0000 00:00:1730914070.714053   10310 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿GPU disponible?: []\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../') \n",
    "from utils.loader import Loader\n",
    "from utils.evaluator import Evaluator\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, Input, Concatenate, LSTM, Bidirectional, Attention, Flatten\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Descargar la lista de stopwords si no está ya descargada\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Cargar las stopwords en español\n",
    "spanish_stopwords = set(stopwords.words('spanish'))\n",
    "\n",
    "\n",
    "print(\"¿GPU disponible?:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingfeng/Desktop/repos/Cars_Price_Prediction/NLP/../utils/loader.py:153: DtypeWarning: Columns (120) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path,\n",
      "/home/lingfeng/Desktop/repos/Cars_Price_Prediction/NLP/../utils/loader.py:153: DtypeWarning: Columns (120) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path,\n",
      "/home/lingfeng/Desktop/repos/Cars_Price_Prediction/NLP/../utils/loader.py:165: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df[\"km\"].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "train = Loader.load_NLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10310/2940014903.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "def custom_concat(row, cols):\n",
    "    # Construir la descripción con lógica condicional basada en el valor de la celda\n",
    "    parts = []\n",
    "    for col_name in cols:  # Cambio para iterar solo sobre las columnas especificadas\n",
    "        if col_name in row.index:  # Verificar que el nombre de la columna esté en el DataFrame\n",
    "            value = row[col_name]\n",
    "            if value == \"no tiene\" or not isinstance(value, str):\n",
    "                parts.append(f\"no tiene {col_name}\")\n",
    "            else:\n",
    "                parts.append(str(value))  # Convertir a string para evitar problemas al unir\n",
    "    # Unir todas las partes con espacios\n",
    "    return ' '.join(parts)\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "def filter_train_data(train):\n",
    "    descriptions = [col for col in train.columns if \"description\" in col]\n",
    "    train['full_description'] = train.apply(custom_concat, axis=1, args=(descriptions,))\n",
    "    filtered_columns = [\"price\", \"km\", \"fuelType\", \"full_description\"]\n",
    "    train = train[filtered_columns]\n",
    "    train.dropna(inplace=True)\n",
    "    return train\n",
    "\n",
    "train = filter_train_data(train)\n",
    "\n",
    "km_scaler = StandardScaler()\n",
    "train[\"km\"] = km_scaler.fit_transform(train[\"km\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "price_scaler = StandardScaler()\n",
    "train[\"price\"] = price_scaler.fit_transform(train[\"price\"].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing TFBertModel: ['mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert/pooler/dense/kernel:0', 'bert/pooler/dense/bias:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo BETO con capas congeladas\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
    "beto_model = TFAutoModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
    "beto_model.trainable = False  # Congelar todas las capas de BETO\n",
    "\n",
    "# Tokenizar y codificar las descripciones usando BETO\n",
    "encoded_inputs = tokenizer(list(train['full_description']), padding=True, truncation=True, max_length=verb_size, return_tensors=\"tf\")\n",
    "\n",
    "# Crear las entradas de texto usando las IDs de los tokens y las máscaras de atención de BETO\n",
    "text_input_ids = Input(shape=(verb_size,), dtype=tf.int32, name='text_input_ids')\n",
    "text_attention_mask = Input(shape=(verb_size,), dtype=tf.int32, name='text_attention_mask')\n",
    "\n",
    "# Obtener embeddings de BETO sin entrenar\n",
    "beto_outputs = beto_model(text_input_ids, attention_mask=text_attention_mask)\n",
    "beto_embeddings = beto_outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplanar la salida para integrarla con las demás entradas\n",
    "flatten = Flatten()(beto_embeddings)\n",
    "\n",
    "#  KM branch (sin cambios)\n",
    "input_km = Input(shape=(1,), name='km_input')\n",
    "km_processed = Dense(32, activation='linear')(input_km)\n",
    "\n",
    "# Concatenar ambas ramas\n",
    "concat = Concatenate()([flatten, km_processed])\n",
    "hidden = Dense(256, activation='relu')(concat)\n",
    "output = Dense(1, activation='linear')(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo final con BETO congelado y la entrada KM\n",
    "model = Model(inputs=[text_input_ids, text_attention_mask, input_km], outputs=output)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='huber', metrics=['mean_absolute_error', \"mean_absolute_percentage_error\"])\n",
    "\n",
    "# Preparar los datos para el entrenamiento\n",
    "ids = encoded_inputs[\"input_ids\"]\n",
    "attention = encoded_inputs[\"attention_mask\"]\n",
    "\n",
    "train_ids, test_ids, train_attention, test_attention, train_prices, test_prices, train_km, test_km = train_test_split(\n",
    "    ids, attention, train['price'], train['km'], test_size=0.2, random_state=42, stratify=train['fuelType'])\n",
    "\n",
    "train_prices = tf.convert_to_tensor(train_prices, dtype=tf.float32)\n",
    "test_prices = tf.convert_to_tensor(test_prices, dtype=tf.float32)\n",
    "\n",
    "train_km = tf.convert_to_tensor(train_km, dtype=tf.float32)\n",
    "train_km = tf.convert_to_tensor(train_km, dtype=tf.float32)\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit([train_ids, train_attention, train_km], train_prices, validation_data=([test_ids, test_attention, test_km], test_prices), epochs=30, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
