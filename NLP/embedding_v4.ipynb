{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../') \n",
    "from utils.loader import Loader\n",
    "from utils.evaluator import Evaluator\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torchmetrics import MeanSquaredError\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments, BertModel\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Descargar la lista de stopwords si no está ya descargada\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Cargar las stopwords en español\n",
    "spanish_stopwords = set(stopwords.words('spanish'))\n",
    "\n",
    "\n",
    "print(\"¿GPU disponible?:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Loader.load_NLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_concat(row, cols):\n",
    "    # Construir la descripción con lógica condicional basada en el valor de la celda\n",
    "    parts = []\n",
    "    for col_name in cols:  # Cambio para iterar solo sobre las columnas especificadas\n",
    "        if col_name in row.index:  # Verificar que el nombre de la columna esté en el DataFrame\n",
    "            value = row[col_name]\n",
    "            if value == \"no tiene\" or not isinstance(value, str):\n",
    "                parts.append(f\"no tiene {col_name}\")\n",
    "            else:\n",
    "                parts.append(str(value))  # Convertir a string para evitar problemas al unir\n",
    "    # Unir todas las partes con espacios\n",
    "    return ' '.join(parts)\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "def filter_train_data(train):\n",
    "    descriptions = [col for col in train.columns if \"description\" in col]\n",
    "    train['full_description'] = train.apply(custom_concat, axis=1, args=(descriptions,))\n",
    "    filtered_columns = [\"price\", \"km\", \"fuelType\", \"full_description\"]\n",
    "    train = train[filtered_columns]\n",
    "    train.dropna(inplace=True)\n",
    "    return train\n",
    "\n",
    "train = filter_train_data(train)\n",
    "\n",
    "km_scaler = StandardScaler()\n",
    "train[\"km\"] = km_scaler.fit_transform(train[\"km\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "price_scaler = StandardScaler()\n",
    "train[\"price\"] = price_scaler.fit_transform(train[\"price\"].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_size = 128\n",
    "model_name = 'dccuchile/bert-base-spanish-wwm-cased'  # BETO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels, train_km, val_km = train_test_split(\n",
    "    train[\"full_description\"],\n",
    "    train[\"price\"],\n",
    "    train[\"km\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(\n",
    "    list(train_texts), truncation=True, padding=True, max_length=verb_size\n",
    ")\n",
    "val_encodings = tokenizer(\n",
    "    list(val_texts), truncation=True, padding=True, max_length=verb_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, encodings, labels, km_values):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels.astype(np.float32)  # Ensure labels are float\n",
    "        self.km_values = torch.tensor(km_values, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx])\n",
    "        item['km'] = self.km_values[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RegressionDataset(train_encodings, train_labels, train_km.values)\n",
    "val_dataset = RegressionDataset(val_encodings, val_labels, val_km.values)\n",
    "\n",
    "class CustomRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, bert_model_name, km_dim=1):\n",
    "        super(CustomRegressionModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.km_layer = torch.nn.Linear(km_dim, 16)  # Process km separately\n",
    "        self.regressor = torch.nn.Linear(self.bert.config.hidden_size + 16, 1)  # Combine BERT and km outputs\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, km, labels=None):\n",
    "        # BERT outputs\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        bert_cls_output = outputs.pooler_output  # [CLS] token representation\n",
    "\n",
    "        # Process km\n",
    "        if len(km.shape) == 1:\n",
    "            km = km.unsqueeze(1)  # Ensure km has two dimensions\n",
    "        km_output = self.km_layer(km)\n",
    "\n",
    "        # Concatenate and pass to regression head\n",
    "        combined_output = torch.cat((bert_cls_output, km_output), dim=1)\n",
    "        logits = self.regressor(combined_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "            loss = loss_fn(logits.view(-1), labels.view(-1))\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits, \"labels\": labels}\n",
    "    \n",
    "model = CustomRegressionModel(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_metric = MeanSquaredError()\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    # Unpack predictions and ensure it's a tensor\n",
    "    predictions = torch.tensor(pred.predictions[0]).flatten()  # Access first element of the tuple and flatten\n",
    "    labels = torch.tensor(pred.label_ids)  # Labels are directly accessible\n",
    "\n",
    "    # Compute Mean Squared Error\n",
    "    mse = mean_squared_error(labels, predictions)\n",
    "\n",
    "    return {\"mse\": mse}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy='epoch',  # Evaluar al final de cada época\n",
    "    save_strategy='epoch',  # Guardar modelo al final de cada época\n",
    "    logging_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_mse',  # Usar eval_loss como métrica principal\n",
    "    disable_tqdm=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "output_model_dir = './final_model'\n",
    "trainer.save_model(output_model_dir)\n",
    "metrics = trainer.evaluate()\n",
    "print(\"Returned metrics:\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar predicciones con el conjunto de validación\n",
    "predictions = trainer.predict(val_dataset)\n",
    "\n",
    "predicted_values = price_scaler.inverse_transform(predictions.predictions[0])  \n",
    "true_values = price_scaler.inverse_transform(val_labels.values.reshape(-1, 1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_values.shape)\n",
    "print(true_values.shape)\n",
    "Evaluator.eval_regression(y_pred=np.array(predicted_values.ravel()), y_true=np.array(true_values.ravel()), plot=False, n_features=2, regressor_name=\"BETO\")\n",
    "Evaluator.save(\"BETO\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
