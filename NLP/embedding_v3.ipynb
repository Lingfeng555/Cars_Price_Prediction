{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../') \n",
    "from utils.loader import Loader\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten, Input, Concatenate, BatchNormalization, LSTM\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Descargar la lista de stopwords si no está ya descargada\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Cargar las stopwords en español\n",
    "spanish_stopwords = set(stopwords.words('spanish'))\n",
    "\n",
    "\n",
    "print(\"¿GPU disponible?:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Loader.load_NLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_concat(row, cols):\n",
    "    # Construir la descripción con lógica condicional basada en el valor de la celda\n",
    "    parts = []\n",
    "    for col_name in cols:  # Cambio para iterar solo sobre las columnas especificadas\n",
    "        if col_name in row.index:  # Verificar que el nombre de la columna esté en el DataFrame\n",
    "            value = row[col_name]\n",
    "            if value == \"no tiene\" or not isinstance(value, str):\n",
    "                parts.append(f\"no tiene {col_name}\")\n",
    "            else:\n",
    "                parts.append(str(value))  # Convertir a string para evitar problemas al unir\n",
    "    # Unir todas las partes con espacios\n",
    "    return ' '.join(parts)\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "def filter_train_data(train):\n",
    "    descriptions = [col for col in train.columns if \"description\" in col]\n",
    "    train['full_description'] = train.apply(custom_concat, axis=1, args=(descriptions,))\n",
    "    filtered_columns = [\"price\", \"km\", \"fuelType\", \"full_description\"]\n",
    "    train = train[filtered_columns]\n",
    "    train.dropna(inplace=True)\n",
    "    return train\n",
    "\n",
    "train = filter_train_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "verb_size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (assuming df is your DataFrame containing the required columns)\n",
    "texts = train['full_description'].values  # descriptions\n",
    "# Tokenization and sequence padding\n",
    "tokenizer = Tokenizer(num_words=verb_size)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences,  maxlen=verb_size, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_prices, test_prices, train_km, test_km = train_test_split(\n",
    "    data, train['price'], train['km'], test_size=0.2, random_state=42, stratify=train['fuelType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_prices.shape)\n",
    "print(train['km'].shape)\n",
    "print(train_km.shape)\n",
    "print(test_km.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.convert_to_tensor(train_data, dtype=tf.int32)\n",
    "test_data = tf.convert_to_tensor(test_data, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prices = tf.convert_to_tensor(train_prices, dtype=tf.float32)\n",
    "test_prices = tf.convert_to_tensor(test_prices, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_km = tf.convert_to_tensor(train_km, dtype=tf.float32)\n",
    "train_km = tf.convert_to_tensor(train_km, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text input branch\n",
    "text_input = Input(shape=(train_data.shape[1],), dtype='int32')\n",
    "text_embed_layer = Embedding(input_dim=train_data.shape[1], output_dim=verb_size, input_length=train_data.shape[1])\n",
    "text_embed = text_embed_layer(text_input)\n",
    "lstm_out = LSTM(128)(text_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding input branch\n",
    "\n",
    "# KM branch\n",
    "input_km = Input(shape=(1,), name='km_input')\n",
    "km_processed = Dense(32, activation='linear')(input_km)\n",
    "\n",
    "# Concatenate both branches\n",
    "concat = Concatenate()([lstm_out, km_processed])\n",
    "hidden = Dense(256, activation='relu')(concat)\n",
    "hidden2 = Dense(128, activation='relu')(hidden)\n",
    "hidden3 = Dense(32, activation='relu')(hidden2)\n",
    "hidden4 = Dense(16, activation='relu')(hidden3)\n",
    "output = Dense(1, activation='linear')(hidden3)  # Output layer for price prediction\n",
    "\n",
    "model = Model(inputs=[text_input, input_km], outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mean_absolute_error', metrics=['mean_absolute_error', \"mean_absolute_percentage_error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"COMIENZA EL ENTRENAMIENTO...\")\n",
    "history = model.fit([train_data, train_km], train_prices,\n",
    "          validation_data=([test_data, test_km], test_prices),\n",
    "          epochs=200, batch_size=32)\n",
    "print(\"TERMINA EL ENTRENAMIENTO...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"# Epoca\")\n",
    "plt.ylabel(\"Loss magnitude\")\n",
    "plt.plot(history.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_prices = model.predict([test_data, test_km])\n",
    "\n",
    "# Assuming test_prices is a 1D array, we can convert it to a DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual Price': test_prices,\n",
    "    'Predicted Price': predicted_prices.flatten(),  # Flatten in case it's a 2D array\n",
    "    'km' : test_km\n",
    "})\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(results_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
